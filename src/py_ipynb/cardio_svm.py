# -*- coding: utf-8 -*-
"""cardio_SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oy0MyMKC2Hvx984c9dM9lx-MOakHBS-T
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.preprocessing

#This line loads our data file to the current working enviroment. From its storage location and into a dataframe.

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/cardio_train.csv', delimiter=';',index_col='id')
df.head()

# To display a summary of the data in our dataframe, giving useful information we could use to spot some issues early on.
# Notice that the age is in days, which is not typical.
# The systolic blood pressure 'ap_hi' has a max value of 16020 in millimeters of mecury (mmHg) which is not a possible value for human blood pressure.
# The diastolic blood pressure 'ap_low' has a max value of 11000 mmHg, this is also not possible. Using describe() helps us spot these early on.

df.describe()

#just to get a list of the columns we are working with at the moment. We will make some adjustments.

df.columns

# We have used .loc[] to identify the columns with a systolic BP >500mmHg and displayed these in a descriptive dataframe (there are 38).

df.loc[df.ap_hi>500].describe()

# We will drop the columns that have blood pressures which are unrealistically high.
# In statistics these are outliers. They tend to have an outsized influence of the outcome of our calculations if we leave them in.
# We take these entries out of our dataset as part of data-cleaning, but such decisions need to be justifiable.

bp_data = df.drop(df[(df['ap_hi'] > 500) | (df['ap_hi'] < 0) | (df['ap_lo'] > 300) | (df['ap_lo'] < 0)].index)

# Age is typically quoted in years, this decision is more to enable better readability for us humans.

bp_data.loc[:, 'age'] = bp_data['age'] / 365.25
bp_data.describe()

# We often use bmi in clinical decision making as it puts weight in context of a person's height.
# That is the reason why I replaced the weight and height sections with the bmi.

bp_data['bmi'] = bp_data['weight'] / (bp_data['height'] / 100) ** 2
bp_data.describe()

# Rearranging the dataframe after the changes above.

bp_data.drop(['height', 'weight'], axis=1, inplace=True)
bp_data = bp_data[['age', 'bmi', 'gender', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio']]

bp_data.describe()

"""In the code below we train 3 models on the data above. A random forest model, a support vector machine, and a gradient boosting machine.

The heatmap below shows the correlation between different features in the original dataset. The more the correlation approaches 1, the stronger the relationship between those two variables. A correlation of 1 would mean the two variables are ALWAYS seen together. Which is why there is a correlation of 1 running down the diagonal as each variable is perfectly correlated with itself.

On the far-right (and along the bottom) we see the correlation between the features and the presence of cardio vascular disease. Systolic blood pressure appears to have the highest correlation with cardiovascular disease at 0.40. Age is also correlated with an increase in age showing a correlation with cardiovascular disease. Gender has a low correlation with cardiovascular disease.

There is a bit of a surprise in the correlation. Activity is the most negatively correlated with cardiovascular disease, no surprises there as an increase in physical activity is often encouraged as a means to improve health and fitness. The surprise here is the negative correlation between smoking and alcohol with cardiovascular disease, this idea does not follow, it may be a pointer to why we use models to improve our predictions.
"""

correlation_matrix = bp_data.corr()

plt.figure(figsize=(12, 6))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, linecolor='white', linewidths=0.5)

plt.suptitle('Correlation Heatmap', fontsize=12)
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split

X = bp_data.drop('cardio', axis=1)  # Features
y = bp_data['cardio']  # Target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #we are keeping 20% of our data aside to be used to test the model's

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier

# Random forest model
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# SVM model
svm = SVC()
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

# Gradient boosting machine model
gbm = GradientBoostingClassifier()
gbm.fit(X_train, y_train)
y_pred_gbm = gbm.predict(X_test)

"""We will test the performance of these models using four performance metrics and display their performance in a bar chart: accuracy_score, precision_score, recall_score, f1_score."""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

acc = accuracy_score(y_test, y_pred_rf)
print('Random forest accuracy:', acc)

acc = accuracy_score(y_test, y_pred_svm)
print('SVM accuracy:', acc)

acc = accuracy_score(y_test, y_pred_gbm)
print('Gradient boosting machine accuracy:', acc)

# Accuracy
fig, ax = plt.subplots()

sns.barplot(x=['Random Forest', 'SVM', 'Gradient Boosting Machine'], y=[accuracy_score(y_test, y_pred_rf), accuracy_score(y_test, y_pred_svm), accuracy_score(y_test, y_pred_gbm)])

ax.set_title('Accuracy Comparison')
ax.set_xlabel('Model')
ax.set_ylabel('Accuracy Score')
plt.show()

prec = precision_score(y_test, y_pred_rf)
print('Random forest precision:', prec)

prec = precision_score(y_test, y_pred_svm)
print('SVM precision:', prec)

prec = precision_score(y_test, y_pred_gbm)
print('Gradient boosting machine precision:', prec)

# Precision
fig, ax = plt.subplots()

sns.barplot(x=['Random Forest', 'SVM', 'Gradient Boosting Machine'], y=[precision_score(y_test, y_pred_rf), precision_score(y_test, y_pred_svm), precision_score(y_test, y_pred_gbm)])

ax.set_title('Precision Comparison')
ax.set_xlabel('Model')
ax.set_ylabel('Precision Score')
plt.show()

rec = recall_score(y_test, y_pred_rf)
print('Random forest recall:', rec)

rec = recall_score(y_test, y_pred_svm)
print('SVM recall:', rec)

rec = recall_score(y_test, y_pred_gbm)
print('Gradient boosting machine recall:', rec)

# Recall
fig, ax = plt.subplots()

sns.barplot(x=['Random Forest', 'SVM', 'Gradient Boosting Machine'], y=[recall_score(y_test, y_pred_rf), recall_score(y_test, y_pred_svm), recall_score(y_test, y_pred_gbm)])

ax.set_title('Recall Comparison')
ax.set_xlabel('Model')
ax.set_ylabel('Recall Score')
plt.show()

f1 = f1_score(y_test, y_pred_rf)
print('Random forest F1-score:', f1)

f1 = f1_score(y_test, y_pred_svm)
print('SVM F1-score:', f1)

f1 = f1_score(y_test, y_pred_gbm)
print('Gradient boosting machine F1-score:', f1)

# F1-score
fig, ax = plt.subplots()

sns.barplot(x=['Random Forest', 'SVM', 'Gradient Boosting Machine'], y=[f1_score(y_test, y_pred_rf), f1_score(y_test, y_pred_svm), f1_score(y_test, y_pred_gbm)])

ax.set_title('F1-score Comparison')
ax.set_xlabel('Model')
ax.set_ylabel('F1-Score')
plt.show()