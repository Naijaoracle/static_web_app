# -*- coding: utf-8 -*-
"""kerastb01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HYCTCgYJMtcBSv36MVl0Dv7TqFjPkz31
"""

#importing the necessary libraries
import pandas as pd
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from google.colab import drive
drive.mount('/content/drive')

csv_file_path = '/content/drive/MyDrive/Projects /colab et al/tb_labels.csv'

#x-rays
image_dir = '/content/drive/MyDrive/Projects /colab et al/TB_CXR_images'

#x-ray labels
data = pd.read_csv('/content/drive/MyDrive/Projects /colab et al/tb_labels.csv')

data.tail()

#loop to extract the images and labels and add to lists
#where to store the images and the labels from the loaded data
image_paths = []
labels = []
for index, row in data.iterrows():
    filename = row['filename']
    label = row['label']

    #full path to the image
    img_path = os.path.join(image_dir, filename)

    # Check if the image file exists
    if os.path.exists(img_path):
        image_paths.append(img_path)
        labels.append(label)
print(len(labels))
print(len(image_paths))

# Split the data into training, validation, and test sets.
train_files, test_files, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)
train_files, val_files, train_labels, val_labels = train_test_split(train_files, train_labels, test_size=0.1, random_state=42)

# Display the number of samples in each set
print(f"Training samples: {len(train_files)}")
print(f"Validation samples: {len(val_files)}")
print(f"Test samples: {len(test_files)}")

#Function to load and preprocess images
def load_and_preprocess_image(file_path, target_size=(224, 224)):
    img = cv2.imread(file_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB
    img = cv2.resize(img, target_size)  # Resize the image to the target size
    img = img / 255.0  # Normalize pixel values between 0 and 1
    return img

# Example to load and preprocess an image
example_img = load_and_preprocess_image(train_files[0])
print(f"Example image shape: {example_img.shape}")

"""Correct format 224 by 224, RGB"""

# Preprocess all images in the training, validation, and test sets
X_train = np.array([load_and_preprocess_image(file) for file in train_files])
X_val = np.array([load_and_preprocess_image(file) for file in val_files])
X_test = np.array([load_and_preprocess_image(file) for file in test_files])

# Convert labels to np arrays
y_train = np.array(train_labels)
y_val = np.array(val_labels)
y_test = np.array(test_labels)

# We will save the preprocessed data to Drive
output_dir = '/content/drive/MyDrive/Projects/colab et al/keras01_processed_data/'
os.makedirs(output_dir, exist_ok=True)

# Save the processed data and labels as .npy files
np.save(output_dir + 'X_train.npy', X_train)
np.save(output_dir + 'X_val.npy', X_val)
np.save(output_dir + 'X_test.npy', X_test)

np.save(output_dir + 'y_train.npy', y_train)
np.save(output_dir + 'y_val.npy', y_val)
np.save(output_dir + 'y_test.npy', y_test)

"""In case we want to load the saved data later on.

#To load the processed data from .npy files
output_dir = '/content/drive/MyDrive/Projects/colab et al/keras01_processed_data/'
loaded_X_train = np.load(output_dir + 'X_train.npy')
loaded_X_val = np.load(output_dir + 'X_val.npy')
loaded_X_test = np.load(output_dir + 'X_test.npy')

loaded_y_train = np.load(output_dir + 'y_train.npy')
loaded_y_val = np.load(output_dir + 'y_val.npy')
loaded_y_test = np.load(output_dir + 'y_test.npy')

Moving on to training the model after preprocessing the chest xray images
"""

# importing the necessary keras modules
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

# The CNN
model = Sequential()

model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(224, 224, 3), kernel_initializer='he_normal'))  # 5x5 filters, He initialization
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, (5, 5), activation='relu', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(128, (5, 5), activation='relu', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))

model.add(Flatten())

model.add(Dense(512, activation='relu'))
model.add(Dropout(0.6))

model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Training the model on the images
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])

# Evaluate model on the set aside testing data
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc}")

"""70.37% accurate. There has been some improvement compared to the last model. There were many different changes made, so the difference in output is not very explainable. For better explainability it would be better to make one change at a time and assess."""

#saving the weights of the model
model_weights_path = '/content/drive/MyDrive/Projects/colab et al/kerastb01_model_weights.h5'
model.save_weights(model_weights_path)

"""# To load later on
# Define a model with the same architecture
model = Sequential()

# ... Architecture definition goes here ...

# Load
model.load_weights('/content/drive/MyDrive/Projects/colab et al/keras_model_weights.h5')

We can visualise this with the matplotlib library tools. ðŸ“Š
"""

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

